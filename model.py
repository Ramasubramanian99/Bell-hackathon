# -*- coding: utf-8 -*-
"""Bell hackathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BSPa3Gp8b738U1fqavIwCS7f_M8pq9oW
"""

# from google.colab import files
# uploaded = files.upload()

import io
import pandas as pd


df = pd.read_csv('./Crop_recommendation.csv')
# Dataset is now stored in a Pandas Dataframe

df.head()

df.label.unique()

df.shape

# look at missing value ratio in each column
df.isnull().sum()*100/df.shape[0]

df.describe()

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# It is symmetrical and bell shaped, showing that trials will usually give a result near the average, 
# but will occasionally deviate by large amounts. It's also fascinating how these two really resemble each other!

plt.figure(figsize=(12,5))
plt.subplot(1, 2, 1)
# sns.distplot(df_setosa['sepal_length'],kde=True,color='green',bins=20,hist_kws={'alpha':0.3})
sns.distplot(df['temperature'],color="purple",bins=15,hist_kws={'alpha':0.2})
plt.subplot(1, 2, 2)
sns.distplot(df['ph'],color="green",bins=15,hist_kws={'alpha':0.2})

sns.boxplot(y='label',x='ph',data=df)

# We can see ph values are critical when it comes to soil. A stability between 6 and 7 is preffered

sns.boxplot(y='label',x='P',data=df[df['rainfall']>150])

# Another interesting analysis where Phosphorous levels are quite differentiable when it rains heavily (above 150 mm).

df['Soil Type'].unique()

from sklearn.preprocessing import LabelEncoder
# encode the categorical features
categorical_cols = ['cinder', 'black', 'peat', 'yellow', 'laterite']
for col in categorical_cols:
    encoder = LabelEncoder()
    df['soil_type'] = encoder.fit_transform(df['Soil Type'])
    print({l: i for i, l in enumerate(encoder.classes_)})

df.head()


# import required libraries
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
# from imblearn.metrics import sensitivity_specificity_support
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC

# divide data into train and test

cols = ['label', 'Soil Type','N', 'P', 'K']
X = df.drop(cols, axis = 1)
y = df.label
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4, stratify = y)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
knn.score(X_test, y_test)

# fine tuning

k_range = range(1,11)
scores = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors = k)
    knn.fit(X_train, y_train)
    scores.append(knn.score(X_test, y_test))

plt.xlabel('k')
plt.ylabel('accuracy')
plt.scatter(k_range, scores)
plt.vlines(k_range,0, scores, linestyle="dashed")
plt.ylim(0.96,0.99)
plt.xticks([i for i in range(1,11)]);

# Using SVM


from sklearn.svm import SVC

svc_linear = SVC(kernel = 'linear').fit(X_train, y_train)
print("Linear Kernel Accuracy: ",svc_linear.score(X_test,y_test))

svc_rbf = SVC(kernel = 'rbf').fit(X_train, y_train)
print("Rbf Kernel Accuracy: ", svc_rbf.score(X_test,y_test))

svc_poly = SVC(kernel = 'poly').fit(X_train, y_train)
print("Poly Kernel Accuracy: ", svc_poly.score(X_test,y_test))

# some fine tuning

# from sklearn.metrics import accuracy_score
# from sklearn.model_selection import GridSearchCV

# parameters = {'C': np.logspace(-3, 2, 6).tolist(), 'gamma': np.logspace(-3, 2, 6).tolist()}
# # 'degree': np.arange(0,5,1).tolist(), 'kernel':['linear','rbf','poly']

# model = GridSearchCV(estimator = SVC(kernel="linear"), param_grid=parameters, n_jobs=-1, cv=4)
# model.fit(X_train, y_train)

# print(model.best_score_ )
# print(model.best_params_ )

# Fine tuning increases computation here and might be inefficient in some cases.

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)
clf.score(X_test,y_test)

# features which are taken into consideration by decision tree

plt.figure(figsize=(10,4), dpi=80)
c_features = len(X_train.columns)
plt.barh(range(c_features), clf.feature_importances_)
plt.xlabel("Feature importance")
plt.ylabel("Feature name")
plt.yticks(np.arange(c_features), X_train.columns)
plt.show()

y_pred = clf.predict(X_test)

clf.score(X_test,y_pred)

y_pred = svc_linear.predict(X_test)

svc_linear.score(X_test,y_pred)



# # deploy

# !pip install pycaret
# !pip install Flask

import pickle

# save the model to disk
filename = 'finalized_model.sav'
pickle.dump(svc_linear, open(filename, 'wb'))







X_train.head()

df.columns

['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall', 'soil_type']